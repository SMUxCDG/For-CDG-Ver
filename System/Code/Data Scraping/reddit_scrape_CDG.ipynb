{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Relevant IDs for Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the client ID, client secret, and user agent as per stated in the user guide, fill in the details in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='', \n",
    "                     client_secret='',\n",
    "                     user_agent='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Function to scrape reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scape_comment(subreddit, keywords) takes 2 elements, subreddit and keywords.\n",
    "\n",
    "Subreddit refers to the subreddit page you wish to scrape from.\n",
    "Keywords refers to the keywords you wish to use to scrape in the stated subreddit page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(subreddit, keyword):\n",
    "    sub = reddit.subreddit(subreddit)\n",
    "\n",
    "    sub_dict = {}\n",
    "    sub_lst = []\n",
    "    sub_comments = []\n",
    "    comment_dict = {}    \n",
    "\n",
    "    for submission in sub.search(keyword, limit = None):\n",
    "        sub_dict['title'] = submission.title\n",
    "        sub_dict['time created'] = pytz.utc.localize(datetime.utcfromtimestamp(submission.created_utc)).astimezone(pytz.timezone(\"Asia/Singapore\")).strftime(\"%d/%m/%Y\")\n",
    "        sub_dict['score']= submission.score\n",
    "        sub_dict['id'] = submission.id\n",
    "        sub_dict['url']= submission.url\n",
    "\n",
    "        submission.comments.replace_more(limit = None) \n",
    "        for comment in submission.comments.list():\n",
    "            comment_dict['time created'] = pytz.utc.localize(datetime.utcfromtimestamp(comment.created_utc)).astimezone(pytz.timezone(\"Asia/Singapore\")).strftime(\"%d/%m/%Y\")\n",
    "            comment_dict['author'] = str(comment.author)\n",
    "            comment_dict['score'] = comment.score\n",
    "            comment_dict['comment'] = comment.body\n",
    "            sub_comments.append(comment_dict)\n",
    "            comment_dict = {}\n",
    "        sub_dict['comments'] = sub_comments\n",
    "\n",
    "        sub_comments = []\n",
    "        sub_lst.append(sub_dict)\n",
    "        sub_dict = {}\n",
    "        comment_dict = {}\n",
    "\n",
    "    # The code below write the file to current directory\n",
    "    # Change the path file to the relevant data file you wish to save to\n",
    "    with open(f\"{subreddit}_{keyword}.json\", 'w', encoding = 'utf-8') as file:\n",
    "        file.write(json.dumps(sub_lst, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the relevant reddit page and keywords you wish to scrape and run the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State the subreddit page and keywords for the function\n",
    "scrape_comments('subreddit page', 'keywords')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
