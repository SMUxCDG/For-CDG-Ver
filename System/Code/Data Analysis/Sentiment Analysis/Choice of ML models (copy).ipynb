{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of ML models For text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../../Data/Scrapping/Reddit/Twitter_Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):  \n",
    "    # Convert text to lowercase\n",
    "    data['clean_text'] = data['clean_text'].str.strip().str.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original tweet -> when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
      "\n",
      "Processed tweet -> ['modi', 'promis', 'minimum', 'govern', 'maximum', 'govern', 'expect', 'begin', 'difficult', 'job', 'reform', 'state', 'take', 'year', 'get', 'justic', 'state', 'busi', 'exit', 'psu', 'templ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivyha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re    # RegEx for removing non-letter characters\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "def tweet_to_words(tweet):\n",
    "    ''' Convert tweet text into a sequence of words '''\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = tweet.lower()\n",
    "    # remove non letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    # tokenize\n",
    "    words = text.split()\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # apply stemming\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    # return list\n",
    "    return words\n",
    "\n",
    "print(\"\\nOriginal tweet ->\", data['clean_text'][0])\n",
    "print(\"\\nProcessed tweet ->\", tweet_to_words(data['clean_text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category_uint8'] = data['category'].astype(np.uint8, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category'] = data['category'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_uint8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category  category_uint8\n",
       "0  when modi promised “minimum government maximum...        -1             255\n",
       "1  talk all the nonsense and continue all the dra...         0               0\n",
       "2  what did just say vote for modi  welcome bjp t...         1               1\n",
       "3  asking his supporters prefix chowkidar their n...         1               1\n",
       "4  answer who among these the most powerful world...         1               1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nb = data['clean_text']\n",
    "y_nb = data['category']\n",
    "x_nb_train, x_nb_test, y_nb_train, y_nb_test = train_test_split(x_nb, y_nb, stratify=y_nb, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text reviews to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.55 GiB for an array with shape (122226, 5000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dc08f83b5f72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_nb_train_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_nb_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_nb_test_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_nb_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.55 GiB for an array with shape (122226, 5000) and data type int64"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(stop_words='english', ngram_range = (1,5), max_features = 5000)\n",
    "x_nb_train_vec = vec.fit_transform(x_nb_train).toarray()\n",
    "x_nb_test_vec = vec.transform(x_nb_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(x_nb_train_vec, y_nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  73.53 % \n",
      "Precision:  73.53 % \n",
      "Recall:  73.53 % \n",
      "F1 Score:  73.53 %\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_nb_test_vec)\n",
    "acc = accuracy_score(y_nb_test, y_pred)\n",
    "f1 = f1_score(y_nb_test, y_pred, average='micro')\n",
    "prec = precision_score(y_nb_test, y_pred, average='micro')\n",
    "rec = recall_score(y_nb_test, y_pred, average='micro')\n",
    "\n",
    "print('Accuracy: ', round(100 * acc, 2),'%',\n",
    "      '\\nPrecision: ', round(100 * prec, 2),'%',\n",
    "      '\\nRecall: ', round(100 * rec, 2),'%',\n",
    "      '\\nF1 Score: ', round(f1*100, 2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm, test_svm = train_test_split(data, test_size=0.2, random_state=1)\n",
    "x_train_svm = train_svm['clean_text'].values\n",
    "x_test_svm = test_svm['clean_text'].values\n",
    "y_train_svm = train_svm['category']\n",
    "y_test_svm = test_svm['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def stem(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "pipeline_svm = make_pipeline(vectorizer, \n",
    "                            SVC(probability=True, kernel=\"linear\", class_weight=\"balanced\"))\n",
    "\n",
    "grid_svm = GridSearchCV(pipeline_svm,\n",
    "                    param_grid = {'svc__C': [0.01, 0.1, 1]}, \n",
    "                    cv = kfolds,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    verbose=1,   \n",
    "                    n_jobs=-1) \n",
    "\n",
    "grid_svm.fit(x_train_svm, y_train_svm)\n",
    "grid_svm.score(x_test_svm, y_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm_proba = model.predict_proba(x_test_svm)[:, 1]\n",
    "y_pred_svm = model.predict(x_test_svm)\n",
    "\n",
    "auc = roc_auc_score(y_test_svm, y_pred_svm_proba)\n",
    "acc = accuracy_score(y_test_svm, y_pred_svm)\n",
    "f1 = f1_score(y_test_svm, y_pred_svm)\n",
    "prec = precision_score(y_test_svm, y_pred_svm)\n",
    "rec = recall_score(y_test_svm, y_pred_svm)\n",
    "\n",
    "print('Accuracy: ', round(100 * acc, 2),'%',\n",
    "      '\\nPrecision: ', round(100 * prec, 2),'%',\n",
    "      '\\nRecall: ', round(100 * rec, 2),'%',\n",
    "      '\\nF1 Score: ', round(f1*100, 2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "\n",
    "acc, f1, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lr = data['clean_text']\n",
    "y_lr =data['category']\n",
    "\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_lr, y_lr, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(min_df=2, ngram_range=(1, 1))\n",
    "x_train_lr = vect.fit(x_train_lr).transform(x_train_lr) \n",
    "x_test_lr = vect.transform(x_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c_val = [0.75, 1, 2, 3, 4, 5, 10]\n",
    "\n",
    "for c in c_val:\n",
    "    logreg = LogisticRegression(C=c)\n",
    "    logreg.fit(x_train_lr, y_train_lr)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test_lr, logreg.predict(x_test_lr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = clf.predict(x_test_lr)\n",
    "acc_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
    "f1 = f1_score(y_test_lr, y_pred_lr, average = 'micro')\n",
    "prec = precision_score(y_test_lr, y_pred_lr, average = 'micro)\n",
    "rec = recall_score(y_test_lr, y_pred_lr, average = 'micro)\n",
    "\n",
    "print('Accuracy: ', round(100 * acc, 2),'%',\n",
    "      '\\nPrecision: ', round(100 * prec, 2),'%',\n",
    "      '\\nRecall: ', round(100 * rec, 2),'%',\n",
    "      '\\nF1 Score: ', round(f1*100, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ivyha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 90720, 'TP': 72249}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-1e9e228ff42e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TN'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconf_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TN'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Shuffle data, not really necesary, just for healthy practice\n",
    "df_slice = data.sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "# Create prediction column based on Polarity Score\n",
    "# -1: Negative Compound Scores\n",
    "# 1: Positive Compound Scores\n",
    "df_slice['Prediction'] = df_slice['clean_text'].apply(lambda x: 1 if sia.polarity_scores(x)['compound'] >= 0.5 else(-1 if sia.polarity_scores(x)['compound'] <= -0.5  else 0))\n",
    "\n",
    "# Edit category column: 1 for positive, -1 or Negative , 0 for Neutral\n",
    "#df_slice['category'] = df_slice['category'].apply(lambda x: -1 if x == \"Negative\" else(1 if x == \"Positive\" else 0))\n",
    "\n",
    "# Check if Category and prediction column match for accuracy calculation\n",
    "df_slice['Accuracy'] = df_slice.apply(lambda x: 1 if x[1] == x[2] else 0, axis = 1)\n",
    "\n",
    "# Create confusion matrix\n",
    "def conf_matrix(x):\n",
    "    if x[1] == 1 and x[2] == 1:\n",
    "        return 'TP'\n",
    "    elif x[1] == 1 and x[2] == -1:\n",
    "        return 'FN'\n",
    "    elif x[1] == -1 and x[2] == 1:\n",
    "        return 'FP'\n",
    "    elif x[1] == -1 and x[2] == -1:\n",
    "        return 'TN'\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_slice['Conf_Matrix'] = df_slice.apply(lambda x: conf_matrix(x), axis = 1)\n",
    "df_slice.tail(10)\n",
    "\n",
    "\n",
    "conf_vals = df_slice.Conf_Matrix.value_counts().to_dict()\n",
    "print(conf_vals)\n",
    "\n",
    "accuracy = (conf_vals['TP'] + conf_vals['TN']) / (conf_vals['TP'] + conf_vals['TN'] + conf_vals['FP'] + conf_vals['FN'])\n",
    "precision = conf_vals['TP'] / (conf_vals['TP'] + conf_vals['FP'])\n",
    "recall = conf_vals['TP'] / (conf_vals['TP'] + conf_vals['FN'])\n",
    "score = f1_score(precision, recall)\n",
    "print('Accuracy: ', round(100 * accuracy, 2),'%',\n",
    "      '\\nPrecision: ', round(100 * precision, 2),'%',\n",
    "      '\\nRecall: ', round(100 * recall, 2),'%',\n",
    "      '\\nF1 Score: ', round(score, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing rows\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Map tweet categories\n",
    "df['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n",
    "# Output first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization & Padding \n",
      " when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
      "After Tokenization & Padding \n",
      " [  42    1  307   66 1726 1119   40 2378    2 1211  205    2  215   32\n",
      "  155  100   49   69 1068  215   50    3    6  546    3   50 4179    3\n",
      " 2806    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Apply data processing to each tweet\n",
    "X = list(map(tweet_to_words, df['clean_text']))\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "\n",
    "def tokenize_pad_sequences(text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    # Text tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Transforms text to a sequence of integers\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return X, tokenizer\n",
    "\n",
    "print('Before Tokenization & Padding \\n', df['clean_text'][0])\n",
    "X, tokenizer = tokenize_pad_sequences(df['clean_text'])\n",
    "print('After Tokenization & Padding \\n', X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set -> (114078, 50) (114078, 3)\n",
      "Validation Set -> (1000, 50) (1000, 3)\n",
      "Test Set -> (113078, 50) (113078, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variable into dummy/indicator variables.\n",
    "y = pd.get_dummies(df['category'])\n",
    "# Train and Test split\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "# Extracting validation set from the train set\n",
    "valid_size=1000\n",
    "X_valid, y_valid = X_train[-valid_size:], y_train[-valid_size:]\n",
    "X_test, y_test = X_train[:-valid_size], y_train[:-valid_size]\n",
    "\n",
    "print('Train Set ->', X_train.shape, y_train.shape)\n",
    "print('Validation Set ->', X_valid.shape, y_valid.shape)\n",
    "print('Test Set ->', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    ''' Function to calculate f1 score '''\n",
    "    \n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 32)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 50, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 179,939\n",
      "Trainable params: 179,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "3565/3565 [==============================] - 56s 15ms/step - loss: 0.3106 - accuracy: 0.8967 - precision: 0.9163 - recall: 0.8778 - val_loss: 0.1886 - val_accuracy: 0.9420 - val_precision: 0.9438 - val_recall: 0.9410\n",
      "\n",
      "CNN + LSTM Accuracy  : 94.81 %\n",
      "CNN + LSTM Precision : 94.98 %\n",
      "CNN + LSTM Recall    : 94.62 %\n",
      "CNN + LSTM F1 Score  : 94.80 %\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_size = 32\n",
    "\n",
    "# Build model\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(Bidirectional(LSTM(32)))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "\n",
    "print(model3.summary())\n",
    "\n",
    "# Compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "               metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Train model\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "history3 = model3.fit(X_train, y_train,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "# Evaluate model on the test set\n",
    "loss, accuracy, precision, recall = model3.evaluate(X_test, y_test, verbose=0)\n",
    "# Print metrics\n",
    "print('')\n",
    "print('CNN + LSTM Accuracy  : {:.2f}'.format(100 * accuracy), '%')\n",
    "print('CNN + LSTM Precision : {:.2f}'.format(100 * precision), '%')\n",
    "print('CNN + LSTM Recall    : {:.2f}'.format(100 * recall), '%')\n",
    "print('CNN + LSTM F1 Score  : {:.2f}'.format(f1_score(precision, recall)), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
